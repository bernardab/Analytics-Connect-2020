{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analytics Connect 2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVnnAsjwJGBSF5mz9/DgLg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bernardab/Analytics-Connect-2020/blob/master/Analytics_Connect_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szSXJzU2lg5G",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Practical Deep Learning with Python\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUGCShGslx5f",
        "colab_type": "text"
      },
      "source": [
        "# SUPERVISED CLASSIFICATION FROM SCRATCH\n",
        "## [Useful when you have lots of data with corresponding labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGQe7cLiLCAw",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch and Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgV2O1Y0lQbR",
        "colab_type": "code",
        "outputId": "3779ed4e-1c3a-48cc-a45e-7875a66ad052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn # layers with trainable parameters\n",
        "import torch.nn.functional as F  # arithmetic operations \n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "torch.manual_seed(8451)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device: {}'.format(torch.cuda.get_device_name(device=device) if \n",
        "                                device.type == 'cuda' else 'cpu'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uflq7JwlifI",
        "colab_type": "code",
        "outputId": "56720383-e775-486b-d5fd-eff605c544c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Tensors are a generalaization of matrices to N dimensions]\n",
        "\n",
        "# vector: 1d tensor\n",
        "V = torch.tensor([1,2,3,4,5])\n",
        "print('vector: {}\\n'.format(V))\n",
        "\n",
        "# matrix: 2d tensor\n",
        "M = torch.tensor([[1., 2., 3.], [4., 5., 6]])\n",
        "print('matrix: {}\\n'.format(M))\n",
        "\n",
        "# 3d tensor of size 2x2x2.\n",
        "T_data = [[[1., 2.], [3., 4.]],\n",
        "          [[5., 6.], [7., 8.]]]\n",
        "\n",
        "T = torch.tensor(T_data)\n",
        "print('3D tensor: {}'.format(T))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vector: tensor([1, 2, 3, 4, 5])\n",
            "\n",
            "matrix: tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "3D tensor: tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uBrOaE4lswQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmOvhQbTl6jW",
        "colab_type": "text"
      },
      "source": [
        "## Load and Transform CIFAR-10 Data using Torchvision\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj7y6KwVl3nh",
        "colab_type": "code",
        "outputId": "fc986f6e-2ade-4183-8c0b-2c2b0461c9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "# Load and transform the Canadian Institute for Advanced Research CIFAR10 training and validation datasets using torchvision\n",
        "## to load custom images and apply custom transforms see, \n",
        "## https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "\n",
        "# define data transforms (e.g normalization)\n",
        "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "# torchvision.transforms.Normalize(mean, std, inplace=False)\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
        "\n",
        "# load CiFAR10 training set \n",
        "num_train_samples = 5000\n",
        "num_validation_samples = 2500\n",
        "batch_size = 4\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2, \n",
        "                                          sampler=torch.utils.data.sampler.SubsetRandomSampler(range(num_train_samples)))\n",
        "\n",
        "validation_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "validationloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2, \n",
        "                                         sampler=torch.utils.data.sampler.SubsetRandomSampler(range(num_validation_samples)))\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# show images\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "data_iter = iter(trainloader)\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAgImFL8P+fyZFJneaynAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " ship  frog  deer  ship\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLpcyH8dnApt",
        "colab_type": "text"
      },
      "source": [
        "## Define a Convolutional Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eVfXdRql7B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://cs231n.github.io/convolutional-networks/\n",
        "# layers:\n",
        "## torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "## torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
        "## torch.nn.Linear(in_features, out_features, bias=True)\n",
        "## torch.nn.ReLU(inplace=False) -  max(0,x)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Oarun_nG6B",
        "colab_type": "text"
      },
      "source": [
        "## Network Print / Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51bEKSsVnBQk",
        "colab_type": "code",
        "outputId": "d3e65051-c0ad-411c-dcfb-135942d35dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "net = Net()\n",
        "net.to(device)\n",
        "print(net)\n",
        "summary(net, (3, 32, 32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             456\n",
            "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
            "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
            "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
            "            Linear-5                  [-1, 120]          48,120\n",
            "            Linear-6                   [-1, 84]          10,164\n",
            "            Linear-7                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.31\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owPghr62nZRy",
        "colab_type": "text"
      },
      "source": [
        "## Define Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96P-3s3bnLVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
        "## Softmax: normalizes input vector into probabiity distribution. Generalization of logistic regression to multiple classes\n",
        "## NLLLOSS. negative log-likelihood of the true labels given probabilistic classifier predictions\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "# Stochastic Gradient Descent with momentum\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrZAMrdUngSy",
        "colab_type": "text"
      },
      "source": [
        "## Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8V69Tnncc4",
        "colab_type": "code",
        "outputId": "12a2aa62-604f-4ec5-8f44-b6ca88a68c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# validation accuracy checker\n",
        "def val_accuracy(net, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total\n",
        "\n",
        "# define network, loss, and optimizer\n",
        "net = Net() \n",
        "net.to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "# train network\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # compute validation accuracy\n",
        "    val_acc = val_accuracy(net, validationloader)\n",
        "         \n",
        "    # print statistics every epoch\n",
        "    print('[epoch: {}] training loss: {:.3f}' ' validation accuracy: {} '.format\n",
        "              (epoch + 1, running_loss / (num_train_samples / batch_size), val_acc))\n",
        "    running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch: 1] training loss: 2.303 validation accuracy: 10.52 \n",
            "[epoch: 2] training loss: 2.300 validation accuracy: 11.04 \n",
            "[epoch: 3] training loss: 2.296 validation accuracy: 13.36 \n",
            "[epoch: 4] training loss: 2.290 validation accuracy: 16.0 \n",
            "[epoch: 5] training loss: 2.275 validation accuracy: 16.8 \n",
            "[epoch: 6] training loss: 2.244 validation accuracy: 19.72 \n",
            "[epoch: 7] training loss: 2.181 validation accuracy: 24.8 \n",
            "[epoch: 8] training loss: 2.093 validation accuracy: 26.36 \n",
            "[epoch: 9] training loss: 2.016 validation accuracy: 27.84 \n",
            "[epoch: 10] training loss: 1.970 validation accuracy: 30.88 \n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmvuembob5w",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "Try to get training loss below 1 and improve validation accuracy without changing number of epochs and training / test data. \n",
        "ideas: change batchsize, learning rate, new optimizer, network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEBxii2uomV9",
        "colab_type": "text"
      },
      "source": [
        "# SUPERVISED CLASSIFICATION WITH TRANSFER LEARNING\n",
        "## [Useful when you have little data with corresponding labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCx82hAUMvRo",
        "colab_type": "text"
      },
      "source": [
        "## Install Huggingface Transformers LIbrary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfit2KN5ofOF",
        "colab_type": "code",
        "outputId": "cc6d11c4-f1e6-4ec6-ac65-5c4e37cf4025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# text sequence classification with transformers\n",
        "# references 1. https://github.com/huggingface/transformers\n",
        "#            2. https://mccormickml.com/2019/07/22/BERT-fine-tuning/ \n",
        "\n",
        "# install huggingface transformer library [SoTA NLP transfer learning]\n",
        "## supports both Pytorch and Tensorflow \n",
        "\n",
        "!pip install transformers==2.4.0\n",
        "\n",
        "# import transformers modules\n",
        "from transformers import * "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.4.0 in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (0.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (1.17.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (1.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.0) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.0) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.0) (7.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.0) (2.8)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.0) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.0) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.4.0) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.4.0) (0.15.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t248WRzIv57M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformers has a unified API for various combinations of architectures and pretrained weights. examples\n",
        "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
        "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
        "          (AlbertModel,     AlbertTokenizer,    'albert-base-v2'),\n",
        "          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n",
        "          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n",
        "        ]\n",
        "\n",
        "# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
        "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
        "                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqPH3lMBDMjN",
        "colab_type": "text"
      },
      "source": [
        "## Load SST Data using Torchtext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cm-kU5P09vU",
        "colab_type": "code",
        "outputId": "08dffe56-c13d-4fcb-b9e2-9a422d948455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load and preprocess Stanford Sentiment Treebank (SST) sequence classification data with torchtext\n",
        "## https://github.com/pytorch/text\n",
        "## torchtext.data: Generic data loaders, abstractions, and iterators for text (including vocabulary and word vectors)\n",
        "## torchtext.datasets: Pre-built loaders for common NLP datasets\n",
        "\n",
        "import torchtext.data\n",
        "import torchtext.datasets\n",
        "\n",
        "# set up fields\n",
        "TEXT = torchtext.data.Field() # define datatype with instruction for converting to tensor\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float) # define labels\n",
        "\n",
        "# make splits of the data\n",
        "train, val, test = torchtext.datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "print('Number ot training examples: {}'.format(len(train)))\n",
        "print('Number ot validation examples: {}'.format(len(val)))\n",
        "print('Number ot test examples: {}'.format(len(test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number ot training examples: 8544\n",
            "Number ot validation examples: 1101\n",
            "Number ot test examples: 2210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VdjPAfrD9tl",
        "colab_type": "code",
        "outputId": "3924accd-cc53-4efb-b00e-67a3320f3b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# Generate train and validation sentences and labels for BERT Sequence Classification \n",
        "def prep_text(x):\n",
        "    sentences = [' '.join(word) for word in x.text]\n",
        "    labels = [''.join(word) for word in x.label]\n",
        "    return sentences, labels \n",
        "\n",
        "train_sentences, train_labels_str = prep_text(train)\n",
        "validation_sentences, validation_labels_str = prep_text(val)\n",
        " \n",
        " # convert labels to integers\n",
        "def label2int(label):\n",
        "    if label == 'neutral': return 2\n",
        "    if label == 'positive': return 1\n",
        "    if label == 'negative': return 0\n",
        "\n",
        "train_labels = [label2int(l) for l in train_labels_str]\n",
        "validation_labels = [label2int(l) for l in validation_labels_str]\n",
        "\n",
        "# print train examples\n",
        "print('Sentiment Label Options; {}'.format(set(train_labels_str)))\n",
        "for (s, l, i) in zip(train_sentences[:10], train_labels_str[:10], train_labels[:10]):\n",
        "  print('{}({})\\t {}\\n'.format(l.upper(),i, s))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment Label Options; {'neutral', 'positive', 'negative'}\n",
            "POSITIVE(1)\t The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
            "\n",
            "POSITIVE(1)\t The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\n",
            "\n",
            "POSITIVE(1)\t Singer\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .\n",
            "\n",
            "NEUTRAL(2)\t You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\n",
            "\n",
            "POSITIVE(1)\t Yet the act is still charming here .\n",
            "\n",
            "POSITIVE(1)\t Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\n",
            "\n",
            "POSITIVE(1)\t Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .\n",
            "\n",
            "POSITIVE(1)\t Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness .\n",
            "\n",
            "POSITIVE(1)\t a screenplay more ingeniously constructed than `` Memento ''\n",
            "\n",
            "POSITIVE(1)\t `` Extreme Ops '' exceeds expectations .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuISJgOSgiV",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize and Format Sentences for BERT Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zytb7NLA_vw5",
        "colab_type": "code",
        "outputId": "66b9b30f-f5d3-414a-ef49-41b9036bf350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# define BERT Tokenizer \n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# tokenize and Format Data using BERT Wordpiece Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# tokenization example\n",
        "s = train_sentences[5]\n",
        "print('Input Sentence: {}'.format(s))\n",
        "print('Tokenized Sentence: {}'.format(tokenizer.tokenize(s)))\n",
        "print('Token IDs: {}'.format(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(s))))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence: Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\n",
            "Tokenized Sentence: ['whether', 'or', 'not', 'you', \"'\", 're', 'en', '##light', '##ened', 'by', 'any', 'of', 'der', '##rid', '##a', \"'\", 's', 'lectures', 'on', '`', '`', 'the', 'other', \"'\", \"'\", 'and', '`', '`', 'the', 'self', ',', \"'\", \"'\", 'der', '##rid', '##a', 'is', 'an', 'und', '##enia', '##bly', 'fascinating', 'and', 'playful', 'fellow', '.']\n",
            "Token IDs: [3251, 2030, 2025, 2017, 1005, 2128, 4372, 7138, 6675, 2011, 2151, 1997, 4315, 14615, 2050, 1005, 1055, 8921, 2006, 1036, 1036, 1996, 2060, 1005, 1005, 1998, 1036, 1036, 1996, 2969, 1010, 1005, 1005, 4315, 14615, 2050, 2003, 2019, 6151, 19825, 6321, 17160, 1998, 18378, 3507, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rHA29WACHyh",
        "colab_type": "code",
        "outputId": "30e94b0c-3172-41c3-880d-6b2cf3ca7588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# tokenize all of the training and validation \n",
        "# sentences and map the tokens to their word IDs as required for BERT training \n",
        "\n",
        "def tokenize_sentences(sentence_list):\n",
        "    \n",
        "    input_ids = []\n",
        "    # For every sentence...\n",
        "    for sent in sentence_list:\n",
        "        # `encode` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start [Classification Token]\n",
        "        #   (3) Append the `[SEP]` token to the end. [Seperator Token]\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        encoded_sent = tokenizer.encode(\n",
        "                            sent,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]') \n",
        "                            )\n",
        "        \n",
        "        # Add the encoded sentence to the list.\n",
        "        input_ids.append(encoded_sent)\n",
        "    return input_ids\n",
        "\n",
        "train_input_ids = tokenize_sentences(train_sentences)\n",
        "validation_input_ids = tokenize_sentences(validation_sentences)\n",
        "\n",
        "# Print train sentence 0, now as a list of IDs.\n",
        "print('Original Sentence: ', train_sentences[0])\n",
        "print('Token IDs:', train_input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Sentence:  The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
            "Token IDs: [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR_FUSGOSane",
        "colab_type": "text"
      },
      "source": [
        "## Pad and Truncate Sentences to Desired Max Length \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nAnK2GZRoaL",
        "colab_type": "code",
        "outputId": "cc1f3ae9-4abb-4224-94fc-0b931f7a93c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# length of longest sentence in training data\n",
        "print('Max sentence length: ', max([len(sen) for sen in train_input_ids]))\n",
        "\n",
        "# usually we want to set max length to 128 (next closest power of 2 to max sentence length)\n",
        "# to make things run faster let's try 32\n",
        "MAX_LEN = 32\n",
        "\n",
        "print('Padding/truncating all sentences to %d values...\\n' % MAX_LEN)\n",
        "\n",
        "print('Padding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "\n",
        "train_inputs = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                              value=0, truncating=\"post\", padding=\"post\")\n",
        "validation_inputs = pad_sequences(validation_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                              value=0, truncating=\"post\", padding=\"post\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  80\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAcJbClYS8U3",
        "colab_type": "text"
      },
      "source": [
        "## Build Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-axDaCkRuex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "def create_attention_masks(input_ids):\n",
        "\n",
        "    attention_masks = []\n",
        "\n",
        "    # For each sentence...\n",
        "    for sent in input_ids:\n",
        "        \n",
        "        # Create the attention mask.\n",
        "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]\n",
        "        \n",
        "        # Store the attention mask for this sentence.\n",
        "        attention_masks.append(att_mask)\n",
        "    return attention_masks\n",
        "\n",
        "train_masks = create_attention_masks(train_inputs)\n",
        "validation_masks = create_attention_masks(validation_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4qZ7uTKHO27",
        "colab_type": "text"
      },
      "source": [
        "## Convert Data to Pytorch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY-BxcXBFLTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAXx3XfOyOF",
        "colab_type": "text"
      },
      "source": [
        "## Define BERT Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvp9TSYdUANG",
        "colab_type": "code",
        "outputId": "6e685dff-11d2-494d-8162-e784c5a35c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--3 for positive, negative and neutral\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoRsI-T9PEHi",
        "colab_type": "text"
      },
      "source": [
        "## Define Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTV5ilq3PGDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adam with weight decay\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8wF0PKtWT61",
        "colab_type": "text"
      },
      "source": [
        "## Train Sentiment Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP2DVOqZUJPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Helper function for formatting elapsed times.\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "338e7426-0e63-4264-fe74-9f5b2640d893",
        "id": "ZhLsHK8-Z9Ar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    267.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    267.    Elapsed: 0:00:32.\n",
            "  Batch   120  of    267.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    267.    Elapsed: 0:01:05.\n",
            "  Batch   200  of    267.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    267.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.71\n",
            "  Training epcoh took: 0:01:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    267.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    267.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    267.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    267.    Elapsed: 0:01:06.\n",
            "  Batch   200  of    267.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    267.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:01:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.74\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    267.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    267.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    267.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    267.    Elapsed: 0:01:06.\n",
            "  Batch   200  of    267.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    267.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:01:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    267.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    267.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    267.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    267.    Elapsed: 0:01:05.\n",
            "  Batch   200  of    267.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    267.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:01:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6XKUEhUpByI",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "Try improving Accuracy by adjusting parameters "
      ]
    }
  ]
}